name: Build wheels for Flash Attention

on:
  workflow_dispatch:  # This allows manual triggering
    inputs:
      flash_attn_version:
        description: 'Flash Attention version'
        required: true
        default: '2.7.4.post1'
        type: choice
        options:
          - '2.4.3'
          - '2.5.9'
          - '2.6.3'
          - '2.7.4.post1'
      python_version:
        description: 'Python version'
        required: true
        default: '3.9'
        type: string
      torch_version:
        description: 'PyTorch version'
        required: true
        default: '2.2.0'
        type: string
      cuda_version:
        description: 'CUDA version'
        required: true
        default: '11.8.0'
        type: string
  create:
    tags:
      - "v*"  # Keeping the original trigger as well

jobs:
  build_wheels:
    name: Build wheels
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - name: Maximize build space
        run: |
          df -h
          echo "-----------------------------"
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          df -h
      - name: Set Swap Space
        uses: pierotofy/set-swap-space@master
        with:
          swap-size-gb: 48

      - name: Set variables based on trigger type
        id: set-vars
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "flash_attn_version=${{ github.event.inputs.flash_attn_version }}" >> $GITHUB_OUTPUT
            echo "python_version=${{ github.event.inputs.python_version }}" >> $GITHUB_OUTPUT
            echo "torch_version=${{ github.event.inputs.torch_version }}" >> $GITHUB_OUTPUT
            echo "cuda_version=${{ github.event.inputs.cuda_version }}" >> $GITHUB_OUTPUT
          else
            echo "flash_attn_version=2.7.4.post1" >> $GITHUB_OUTPUT
            echo "python_version=3.9" >> $GITHUB_OUTPUT
            echo "torch_version=2.2.0" >> $GITHUB_OUTPUT
            echo "cuda_version=11.8.0" >> $GITHUB_OUTPUT
          fi

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ steps.set-vars.outputs.python_version }}
      - uses: Jimver/cuda-toolkit@master
        with:
          cuda: ${{ steps.set-vars.outputs.cuda_version }}
          linux-local-args: '["--toolkit"]'
          method: "network"
      - run: sudo apt install -y ninja-build

      - name: Set CUDA and PyTorch versions
        run: |
          echo "MATRIX_CUDA_VERSION=$(echo ${{ steps.set-vars.outputs.cuda_version }} | awk -F \. {'print $1 $2'})" >> $GITHUB_ENV
          echo "MATRIX_TORCH_VERSION=$(echo ${{ steps.set-vars.outputs.torch_version }} | awk -F \. {'print $1 "." $2'})" >> $GITHUB_ENV
          echo "CACHE_KEY=cuda-ext-${{ steps.set-vars.outputs.flash_attn_version }}-py${{ steps.set-vars.outputs.python_version }}-torch${{ steps.set-vars.outputs.torch_version }}-cuda${{ steps.set-vars.outputs.cuda_version }}" >> $GITHUB_ENV

      - name: Install PyTorch ${{ steps.set-vars.outputs.torch_version }}+cu${{ steps.set-vars.outputs.cuda_version }}
        run: |
          pip install -U pip
          pip install wheel setuptools packaging
          pip install --no-cache-dir torch==${{ steps.set-vars.outputs.torch_version }} --index-url https://download.pytorch.org/whl/cu118
          nvcc --version
          python -V
          python -c "import torch; print('PyTorch:', torch.__version__)"
          python -c "import torch; print('CUDA:', torch.version.cuda)"
          python -c "from torch.utils import cpp_extension; print(cpp_extension.CUDA_HOME)"

      - name: Checkout flash-attn
        run: |
          git clone https://github.com/Dao-AILab/flash-attention.git
          cd flash-attention
          git checkout v${{ steps.set-vars.outputs.flash_attn_version }}

      # Add cache steps for CUDA extension build
      - name: Cache CUDA extension build
        uses: actions/cache@v3
        with:
          path: |
            flash-attention/build
            flash-attention/flash_attn.egg-info
            flash-attention/**/*.so
          key: ${{ env.CACHE_KEY }}-${{ hashFiles('flash-attention/csrc/**') }}
          restore-keys: |
            ${{ env.CACHE_KEY }}-

      - name: Build wheels
        run: |
          pip install setuptools==68.0.0 ninja packaging wheel
          export PATH=/usr/local/nvidia/bin:/usr/local/nvidia/lib64:$PATH
          export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
          export MAX_JOBS=$(($(nproc) - 1))
          cd flash-attention
          FLASH_ATTENTION_FORCE_BUILD="TRUE" python setup.py bdist_wheel --dist-dir=dist
          base_wheel_name=$(basename $(ls dist/*.whl | head -n 1))
          wheel_name=$(echo $base_wheel_name | sed "s/${{ steps.set-vars.outputs.flash_attn_version }}/${{ steps.set-vars.outputs.flash_attn_version }}+cu${{ env.MATRIX_CUDA_VERSION }}torch${{ env.MATRIX_TORCH_VERSION }}/")
          mv dist/$base_wheel_name dist/$wheel_name
          echo "wheel_name=$wheel_name" >> $GITHUB_ENV

      - name: Install Test
        run: |
          pip install flash-attention/dist/${{ env.wheel_name }}
          python -c "import flash_attn; print(flash_attn.__version__)"

      # Save wheel as artifact (available for download in GitHub Actions)
      - name: Upload wheel as artifact
        uses: actions/upload-artifact@v3
        with:
          name: flash-attn-${{ steps.set-vars.outputs.flash_attn_version }}-py${{ steps.set-vars.outputs.python_version }}-torch${{ steps.set-vars.outputs.torch_version }}-cuda${{ steps.set-vars.outputs.cuda_version }}
          path: flash-attention/dist/${{ env.wheel_name }}
          retention-days: 7
